{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 671, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tongdan Su, 20754736"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "Main.py as below, in order to show the result in ipynb, here I changed the code for prompt in order to run it in jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting interest points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\ana3\\lib\\site-packages\\ipykernel_launcher.py:67: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Getting features...\n",
      "Done!\n",
      "Matching features...\n",
      "Done!\n",
      "Matches: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\ana3\\lib\\site-packages\\ipykernel_launcher.py:224: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0 total good matches, 39 total bad matches.\n",
      "73.8255033557047% precision\n",
      "73% accuracy (top 100)\n",
      "Vizualizing...\n"
     ]
    }
   ],
   "source": [
    "# Local Feature Stencil Code\n",
    "# Written by James Hays for CS 143 @ Brown / CS 4476/6476 @ Georgia Tech with Henry Hu <henryhu@gatech.edu>\n",
    "# Edited by James Tompkin\n",
    "# Adapted for python by asabel and jdemari1 (2019)\n",
    "import scipy.io as scio \n",
    "import csv\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from skimage.feature import plot_matches\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage import io, filters, feature, img_as_float32\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "#from helpers import cheat_interest_points, evaluate_correspondence\n",
    " \n",
    "# This script\n",
    "# (1) Loads and resizes images\n",
    "# (2) Finds interest points in those images                 (you code this)\n",
    "# (3) Describes each interest point with a local feature    (you code this)\n",
    "# (4) Finds matching features                               (you code this)\n",
    "# (5) Visualizes the matches\n",
    "# (6) Evaluates the matches based on ground truth correspondences\n",
    " \n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    1) Load stuff\n",
    "    There are numerous other image sets in the supplementary data on the\n",
    "    project web page. You can simply download images off the Internet, as\n",
    "    well. However, the evaluation function at the bottom of this script will\n",
    "    only work for three particular image pairs (unless you add ground truth\n",
    "    annotations for other image pairs). It is suggested that you only work\n",
    "    with the two Notre Dame images until you are satisfied with your\n",
    "    implementation and ready to test on additional images. A single scale\n",
    "    pipeline works fine for these two images (and will give you full credit\n",
    "    for this project), but you will need local features at multiple scales to\n",
    "    handle harder cases.\n",
    " \n",
    "    If you want to add new images to test, create a new elif of the same format as those\n",
    "    for notre_dame, mt_rushmore, etc. You do not need to set the eval_file variable unless\n",
    "    you hand create a ground truth annotations. To run with your new images use\n",
    "    python main.py -p <your file name>.\n",
    " \n",
    "    :param file_name: string for which image pair to compute correspondence for\n",
    " \n",
    "        The first three strings can be used as shortcuts to the\n",
    "        data files we give you\n",
    " \n",
    "        1. notre_dame\n",
    "        2. mt_rushmore\n",
    "        3. e_gaudi\n",
    " \n",
    "    :return: a tuple of the format (image1, image2, eval file)\n",
    "    \"\"\"\n",
    " \n",
    "    # Note: these files default to notre dame, unless otherwise specified\n",
    "    image1_file = \"./NotreDame/NotreDame1.jpg\"\n",
    "    image2_file = \"./NotreDame/NotreDame2.jpg\"\n",
    " \n",
    "    eval_file = \"./NotreDame/NotreDameEval.mat\"\n",
    " \n",
    "    if file_name == \"notre_dame\":\n",
    "        pass\n",
    "    elif file_name == \"mt_rushmore\":\n",
    "        image1_file = \"../data/MountRushmore/Mount_Rushmore1.jpg\"\n",
    "        image2_file = \"../data/MountRushmore/Mount_Rushmore2.jpg\"\n",
    "        eval_file = \"../data/MountRushmore/MountRushmoreEval.mat\"\n",
    "    elif file_name == \"e_gaudi\":\n",
    "        image1_file = \"../data/EpiscopalGaudi/EGaudi_1.jpg\"\n",
    "        image2_file = \"../data/EpiscopalGaudi/EGaudi_2.jpg\"\n",
    "        eval_file = \"../data/EpiscopalGaudi/EGaudiEval.mat\"\n",
    " \n",
    "    image1 = img_as_float32(io.imread(image1_file))\n",
    "    image2 = img_as_float32(io.imread(image2_file))\n",
    " \n",
    "    return image1, image2, eval_file\n",
    " \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Reads in the data,\n",
    " \n",
    "    Command line usage: python main.py [-a | --average_accuracy] -p | --pair <image pair name>\n",
    " \n",
    "    -a | --average_accuracy - flag - if specified, will compute your solution's\n",
    "    average accuracy on the (1) notre dame, (2) mt. rushmore, and (3) episcopal\n",
    "    guadi image pairs\n",
    " \n",
    "    -p | --pair - flag - required. specifies which image pair to match\n",
    " \n",
    "    \"\"\"\n",
    " \n",
    "    # create the command line parser\n",
    "#     parser = argparse.ArgumentParser()\n",
    " \n",
    "#     parser.add_argument(\"-a\", \"--average_accuracy\", help=\"Include this flag to compute the average accuracy of your matching.\")\n",
    "#     parser.add_argument(\"-p\", \"--pair\", required=True, help=\"Either notre_dame, mt_rushmore, or e_gaudi. Specifies which image pair to match\")\n",
    " \n",
    "#     args = parser.parse_args()\n",
    " \n",
    "    # (1) Load in the data\n",
    "    image1, image2, eval_file = load_data(\"\")\n",
    " \n",
    "    # You don't have to work with grayscale images. Matching with color\n",
    "    # information might be helpful. If you choose to work with RGB images, just\n",
    "    # comment these two lines\n",
    "    image1 = rgb2gray(image1)\n",
    "    image2 = rgb2gray(image2)\n",
    "     \n",
    "    # make images smaller to speed up the algorithm. This parameter\n",
    "    # gets passed into the evaluation code, so don't resize the images\n",
    "    # except for changing this parameter - We will evaluate your code using\n",
    "    # scale_factor = 0.5, so be aware of this\n",
    "    scale_factor = 0.5\n",
    " \n",
    "    # Bilinear rescaling\n",
    "    image1 = np.float32(rescale(image1, scale_factor))\n",
    "    image2 = np.float32(rescale(image2, scale_factor))\n",
    " \n",
    "    # width and height of each local feature, in pixels\n",
    "    feature_width = 16\n",
    " \n",
    "    # (2) Find distinctive points in each image. See Szeliski 4.1.1\n",
    "    # !!! You will need to implement get_interest_points. !!!\n",
    " \n",
    "    print(\"Getting interest points...\")\n",
    " \n",
    "    # For development and debugging get_features and match_features, you will likely\n",
    "    # want to use the ta ground truth points, you can comment out the precedeing two\n",
    "    # lines and uncomment the following line to do this.\n",
    " \n",
    "    #(x1, y1, x2, y2) = cheat_interest_points(eval_file, scale_factor)\n",
    " \n",
    "    (x1, y1) = get_interest_points(image1, feature_width)\n",
    "    (x2, y2) = get_interest_points(image2, feature_width)\n",
    " \n",
    "    # if you want to view your corners uncomment these next lines!\n",
    " \n",
    "    # plt.imshow(image1, cmap=\"gray\")\n",
    "    # plt.scatter(x1, y1, alpha=0.9, s=3)\n",
    "    # plt.show()\n",
    " \n",
    "    # plt.imshow(image2, cmap=\"gray\")\n",
    "    # plt.scatter(x2, y2, alpha=0.9, s=3)\n",
    "    # plt.show()\n",
    " \n",
    "    print(\"Done!\")\n",
    " \n",
    "    # 3) Create feature vectors at each interest point. Szeliski 4.1.2\n",
    "    print(\"Getting features...\")\n",
    "    image1_features = get_features(image1, x1, y1, feature_width)\n",
    "    image2_features = get_features(image2, x2, y2, feature_width)\n",
    " \n",
    "    print(\"Done!\")\n",
    " \n",
    "    # 4) Match features. Szeliski 4.1.3\n",
    "    print(\"Matching features...\")\n",
    "    matches, confidences = match_features(image1_features, image2_features)\n",
    "     \n",
    "    if len(matches.shape) == 1:\n",
    "        print( \"No matches!\")\n",
    "        return\n",
    "     \n",
    "    print(\"Done!\")\n",
    " \n",
    " \n",
    "    # 5) Visualization\n",
    " \n",
    "    # You might want to do some preprocessing of your interest points and matches\n",
    "    # before visualizing (e.g. only visualizing 100 interest points). Once you\n",
    "    # start detecting hundreds of interest points, the visualization can become\n",
    "    # crowded. You may also want to threshold based on confidence\n",
    " \n",
    "    # visualize.show_correspondences produces a figure that shows your matches\n",
    "    # overlayed on the image pairs. evaluate_correspondence computes some statistics\n",
    "    # about the quality of your matches, then shows the same figure. If you want to\n",
    "    # just see the figure, you can uncomment the function call to visualize.show_correspondences\n",
    " \n",
    "    #matches=matches[:100] \n",
    "    num_pts_to_visualize = matches.shape[0]\n",
    "    print(\"Matches: \" + str(num_pts_to_visualize))\n",
    "    show_correspondences(image1, image2, x1, y1, x2, y2, matches, filename= \"matches.jpg\")\n",
    "\n",
    "    ## 6) Evaluation\n",
    "    # This evaluation function will only work for the Notre Dame, Episcopal\n",
    "    # Gaudi, and Mount Rushmore image pairs. Comment out this function if you\n",
    "    # are not testing on those image pairs. Only those pairs have ground truth\n",
    "    # available.\n",
    "    #\n",
    "    # It also only evaluates your top 100 matches by the confidences\n",
    "    # that you provide.\n",
    "    #\n",
    "    # Within evaluate_correspondences(), we sort your matches in descending order\n",
    "    \n",
    "    num_pts_to_evaluate = matches.shape[0]\n",
    " \n",
    "    evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "        x1, y1, x2, y2, matches, confidences, num_pts_to_visualize)\n",
    "    #print(\"done\")\n",
    "    return\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correspondences(imgA, imgB, X1, Y1, X2, Y2, matches, mode='arrows', filename=None):\n",
    "    '''\n",
    "        Visualizes corresponding points between two images, either as\n",
    "        arrows or dots\n",
    "        mode='dots': Corresponding points will have the same random color\n",
    "        mode='arrows': Corresponding points will be joined by a line\n",
    "        Writes out a png of the visualization if 'filename' is not None.\n",
    "    '''\n",
    "\n",
    "    # generates unique figures so students can\n",
    "    # look at all three at once\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "    if mode == 'dots':\n",
    "        print(\"dot visualization not implemented yet :(\")\n",
    "\n",
    "    else:\n",
    "        kp1 = zip_x_y(Y1, X1)\n",
    "        kp2 = zip_x_y(Y2, X2)\n",
    "        matches = matches.astype(int)\n",
    "        plot_matches(ax, imgA, imgB, kp1, kp2, matches, only_matches=True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, dpi = 300)\n",
    "    else:\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "def zip_x_y(x, y):\n",
    "    zipped_points = []\n",
    "    for i in range(len(x)):\n",
    "        zipped_points.append(np.array([x[i], y[i]]))\n",
    "    return np.array(zipped_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student.py as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, feature, img_as_int\n",
    "from skimage.measure import regionprops\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "\n",
    "def get_interest_points(image, feature_width, sigma=2, k=0.04):\n",
    "    '''\n",
    "    Returns a set of interest points for the input image\n",
    "\n",
    "    (Please note that we recommend implementing this function last and using cheat_interest_points()\n",
    "    to test your implementation of get_features() and match_features())\n",
    "\n",
    "    Implement the Harris corner detector (See Szeliski 4.1.1) to start with.\n",
    "    You do not need to worry about scale invariance or keypoint orientation estimation\n",
    "    for your Harris corner detector.\n",
    "    You can create additional interest point detector functions (e.g. MSER)\n",
    "    for extra credit.\n",
    "\n",
    "    If you're finding spurious (false/fake) interest point detections near the boundaries,\n",
    "    it is safe to simply suppress the gradients / corners near the edges of\n",
    "    the image.\n",
    "\n",
    "    Useful functions: A working solution does not require the use of all of these\n",
    "    functions, but depending on your implementation, you may find some useful. Please\n",
    "    reference the documentation for each function/library and feel free to come to hours\n",
    "    or post on Piazza with any questions\n",
    "\n",
    "        - skimage.feature.peak_local_max\n",
    "        - skimage.measure.regionprops\n",
    "\n",
    "\n",
    "    :params:\n",
    "    :image: a grayscale or color image (your choice depending on your implementation)\n",
    "    :feature_width:\n",
    "\n",
    "    :returns:\n",
    "    :xs: an np array of the x coordinates of the interest points in the image\n",
    "    :ys: an np array of the y coordinates of the interest points in the image\n",
    "\n",
    "    :optional returns (may be useful for extra credit portions):\n",
    "    :confidences: an np array indicating the confidence (strength) of each interest point\n",
    "    :scale: an np array indicating the scale of each interest point\n",
    "    :orientation: an np array indicating the orientation of each interest point\n",
    "\n",
    "    '''\n",
    "    ix = ndimage.sobel(image, 0)\n",
    "    iy = ndimage.sobel(image, 1)\n",
    "    ix2 = ix * ix\n",
    "    iy2 = iy * iy\n",
    "    ixy = ix * iy\n",
    "    ix2 = ndimage.gaussian_filter(ix2, sigma=2)\n",
    "    iy2 = ndimage.gaussian_filter(iy2, sigma=2)\n",
    "    ixy = ndimage.gaussian_filter(ixy, sigma=2)\n",
    "    det = (ix2 * iy2) - (ixy**2)\n",
    "    trace = ix2 + iy2\n",
    "    R = det - k*(trace**2)\n",
    "    #Attempt 1 Min-Max Normalize:\n",
    "    #R= (R-np.min(R))/(np.max(R)-np.min(R))#Normalize R\n",
    "    #Attempt 2 Z-score Normalize\n",
    "    R=stats.zscore(R)\n",
    "    corners = R\n",
    "    threshold = np.mean(R)\n",
    "    mask = [R < threshold]\n",
    "    corners[mask] = 0\n",
    "    keypoints = peak_local_max(corners, threshold_rel=0.2, exclude_border=True,\n",
    "                               num_peaks=2000, min_distance=feature_width//2)\n",
    "    ys = keypoints[:, 0]\n",
    "    xs = keypoints[:, 1]\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def get_features(image, x, y, feature_width):\n",
    "    '''\n",
    "    Returns a set of feature descriptors for a given set of interest points.\n",
    "\n",
    "    (Please note that we reccomend implementing this function after you have implemented\n",
    "    match_features)\n",
    "\n",
    "    To start with, you might want to simply use normalized patches as your\n",
    "    local feature. This is very simple to code and works OK. However, to get\n",
    "    full credit you will need to implement the more effective SIFT-like descriptor\n",
    "    (See Szeliski 4.1.2 or the original publications at\n",
    "    http://www.cs.ubc.ca/~lowe/keypoints/)\n",
    "\n",
    "    Your implementation does not need to exactly match the SIFT reference.\n",
    "    Here are the key properties your (baseline) descriptor should have:\n",
    "    (1) a 4x4 grid of cells, each descriptor_window_image_width/4.\n",
    "    (2) each cell should have a histogram of the local distribution of\n",
    "        gradients in 8 orientations. Appending these histograms together will\n",
    "        give you 4x4 x 8 = 128 dimensions.\n",
    "    (3) Each feature should be normalized to unit length\n",
    "\n",
    "    You do not need to perform the interpolation in which each gradient\n",
    "    measurement contributes to multiple orientation bins in multiple cells\n",
    "    As described in Szeliski, a single gradient measurement creates a\n",
    "    weighted contribution to the 4 nearest cells and the 2 nearest\n",
    "    orientation bins within each cell, for 8 total contributions. This type\n",
    "    of interpolation probably will help, though.\n",
    "\n",
    "    You do not have to explicitly compute the gradient orientation at each\n",
    "    pixel (although you are free to do so). You can instead filter with\n",
    "    oriented filters (e.g. a filter that responds to edges with a specific\n",
    "    orientation). All of your SIFT-like feature can be constructed entirely\n",
    "    from filtering fairly quickly in this way.\n",
    "\n",
    "    You do not need to do the normalize -> threshold -> normalize again\n",
    "    operation as detailed in Szeliski and the SIFT paper. It can help, though.\n",
    "\n",
    "    Another simple trick which can help is to raise each element of the final\n",
    "    feature vector to some power that is less than one.\n",
    "\n",
    "    Useful functions: A working solution does not require the use of all of these\n",
    "    functions, but depending on your implementation, you may find some useful. Please\n",
    "    reference the documentation for each function/library and feel free to come to hours\n",
    "    or post on Piazza with any questions\n",
    "\n",
    "        - skimage.filters (library)\n",
    "\n",
    "\n",
    "    :params:\n",
    "    :image: a grayscale or color image (your choice depending on your implementation)\n",
    "    :x: np array of x coordinates of interest points\n",
    "    :y: np array of y coordinates of interest points\n",
    "    :feature_width: in pixels, is the local feature width. You can assume\n",
    "                    that feature_width will be a multiple of 4 (i.e. every cell of your\n",
    "                    local SIFT-like feature will have an integer width and height).\n",
    "    If you want to detect and describe features at multiple scales or\n",
    "    particular orientations you can add input arguments.\n",
    "\n",
    "    :returns:\n",
    "    :features: np array of computed features. It should be of size\n",
    "            [len(x) * feature dimensionality] (for standard SIFT feature\n",
    "            dimensionality is 128)\n",
    "\n",
    "    '''\n",
    "\n",
    "    if feature_width % 4 != 0:\n",
    "        raise ValueError(\"feature_width must be a multiple of 4.\")\n",
    "    x = np.round(x).astype(int).flatten()\n",
    "    y = np.round(y).astype(int).flatten()\n",
    "    offset = feature_width // 2\n",
    "    descriptors = list()\n",
    "    n_bins = 8\n",
    "\n",
    "    # compute gradients, magnitudes, angles\n",
    "    gradients = np.array(np.gradient(image))\n",
    "    magnitudes = np.linalg.norm(gradients, axis=0)\n",
    "    angles = np.angle(np.arctan2(gradients[0], gradients[1]), deg=True)\n",
    "    for xi, yi in zip(x, y):\n",
    "        crop_magnitudes = magnitudes[yi-offset +\n",
    "                                     1:yi+offset+1, xi-offset+1:xi+offset+1]\n",
    "        crop_angles = angles[yi-offset+1:yi+offset+1, xi-offset+1:xi+offset+1]\n",
    "        if crop_magnitudes.shape != (feature_width, feature_width):\n",
    "            # Crop does not satisfy size constraint, skip keypoint.\n",
    "            continue\n",
    "\n",
    "        # Create SIFT descriptor\n",
    "        patches_magnitudes = np.array(np.hsplit(np.array(\n",
    "            np.hsplit(crop_magnitudes, 4)).reshape(4, -1), 4)).reshape(-1, feature_width)\n",
    "        patches_angles = np.array(np.hsplit(np.array(\n",
    "            np.hsplit(crop_angles, 4)).reshape(4, -1), 4)).reshape(-1, feature_width)\n",
    "        feature_vector = list()\n",
    "        for patch_i in range(patches_magnitudes.shape[0]):\n",
    "            bins = np.digitize(\n",
    "                patches_angles[patch_i], np.arange(0, 360, 360 // n_bins))\n",
    "            bin_vector = np.zeros(n_bins)\n",
    "            for bin_i in range(0, n_bins):\n",
    "                mask = np.array(bins == bin_i).flatten()\n",
    "                bin_vector[bin_i] = np.sum(\n",
    "                    patches_magnitudes[patch_i].flatten()[mask])\n",
    "            feature_vector.append(bin_vector)\n",
    "        feature_vector_norm = (np.array(feature_vector).flatten() /\n",
    "                               np.array(feature_vector).flatten().sum())\n",
    "        descriptors.append(feature_vector_norm)\n",
    "    return np.array(descriptors)\n",
    "\n",
    "\n",
    "\n",
    "def match_features(im1_features, im2_features):\n",
    "    '''\n",
    "    Implements the Nearest Neighbor Distance Ratio Test to assign matches between interest points\n",
    "    in two images.\n",
    "\n",
    "    Please implement the \"Nearest Neighbor Distance Ratio (NNDR) Test\" ,\n",
    "    Equation 4.18 in Section 4.1.3 of Szeliski.\n",
    "\n",
    "    For extra credit you can implement spatial verification of matches.\n",
    "\n",
    "    Please assign a confidence, else the evaluation function will not work. Remember that\n",
    "    the NNDR test will return a number close to 1 for feature points with similar distances.\n",
    "    Think about how confidence relates to NNDR.\n",
    "\n",
    "    This function does not need to be symmetric (e.g., it can produce\n",
    "    different numbers of matches depending on the order of the arguments).\n",
    "\n",
    "    A match is between a feature in im1_features and a feature in im2_features. We can\n",
    "    represent this match as a the index of the feature in im1_features and the index\n",
    "    of the feature in im2_features\n",
    "\n",
    "    Useful functions: A working solution does not require the use of all of these\n",
    "    functions, but depending on your implementation, you may find some useful. Please\n",
    "    reference the documentation for each function/library and feel free to come to hours\n",
    "    or post on Piazza with any questions\n",
    "\n",
    "        - zip (python built in function)\n",
    "\n",
    "    :params:\n",
    "    :im1_features: an np array of features returned from get_features() for interest points in image1\n",
    "    :im2_features: an np array of features returned from get_features() for interest points in image2\n",
    "\n",
    "    :returns:\n",
    "    :matches: an np array of dimension k x 2 where k is the number of matches. The first\n",
    "            column is an index into im1_features and the second column is an index into im2_features\n",
    "    :confidences: an np array with a real valued confidence for each match\n",
    "    '''\n",
    "    distances = cdist(im1_features, im2_features, metric=\"euclidean\")\n",
    "    row = distances.shape[0]\n",
    "    idxs = np.argsort(distances, axis=1)[:, :2]\n",
    "    d1 = [np.arange(row), idxs[:, 0]]\n",
    "    d2 = [np.arange(row), idxs[:, 1]]\n",
    "    NDDR = distances[d1] / distances[d2]\n",
    "    matches = np.stack((d1[0], d1[1]), axis=1)\n",
    "    confidences = 1 - NDDR\n",
    "\n",
    "    return matches, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correspondence(img_A, img_B, ground_truth_correspondence_file,\n",
    "    scale_factor, x1_est, y1_est, x2_est, y2_est, matches, confidences, vis, filename=\"eval_corr.jpg\"):\n",
    " \n",
    "    # Sort the matches by their confidences into descending order of confidence \n",
    "    # (high confidence at low array index)\n",
    "    conf_sorted = -np.sort(-confidences, kind='mergesort')\n",
    "    conf_indices = np.argsort(-confidences, kind='mergesort')\n",
    "    matches = matches[conf_indices,:]\n",
    "    confidences = conf_sorted\n",
    " \n",
    "    # 'unscale' interest points to compare with ground truth points\n",
    "    x1_est_scaled = x1_est / scale_factor\n",
    "    y1_est_scaled = y1_est / scale_factor\n",
    "    x2_est_scaled = x2_est / scale_factor\n",
    "    y2_est_scaled = y2_est / scale_factor\n",
    " \n",
    "    # We want to see how good our matches are;\n",
    "    # extract the coordinates of each matched point\n",
    " \n",
    "    x1_matches = np.zeros(matches.shape[0])\n",
    "    y1_matches = np.zeros(matches.shape[0])\n",
    "    x2_matches = np.zeros(matches.shape[0])\n",
    "    y2_matches = np.zeros(matches.shape[0])\n",
    " \n",
    "    for i in range(matches.shape[0]):\n",
    " \n",
    "        x1_matches[i] = x1_est_scaled[int(matches[i, 0])]\n",
    "        y1_matches[i] = y1_est_scaled[int(matches[i, 0])]\n",
    "        x2_matches[i] = x2_est_scaled[int(matches[i, 1])]\n",
    "        y2_matches[i] = y2_est_scaled[int(matches[i, 1])]\n",
    " \n",
    "    good_matches = np.zeros((matches.shape[0],1))\n",
    " \n",
    "    # Loads `ground truth' positions x1, y1, x2, y2\n",
    "    file_contents = scio.loadmat(ground_truth_correspondence_file)\n",
    " \n",
    "    # x1, y1, x2, y2 = scio.loadmat(eval_file)\n",
    "    x1 = file_contents['x1']\n",
    "    y1 = file_contents['y1']\n",
    "    x2 = file_contents['x2']\n",
    "    y2 = file_contents['y2']\n",
    " \n",
    "    uniqueness_dist = 150\n",
    "    good_match_dist = 150\n",
    " \n",
    "    good_match_counter = 0\n",
    "    bad_match_counter = 0\n",
    "    top_100_counter = 0\n",
    " \n",
    "    # Used to keep track of which TA points the student has matched\n",
    "    # to so the student only gets credit for matching a TA point once\n",
    "    correct_matches = np.zeros(x2.shape[0])\n",
    " \n",
    "    # for each ground truth point in image 1\n",
    "    for i in range(x1.shape[0]):\n",
    " \n",
    "        # 1. find the student points within uniqueness_dist pixels of the ground truth point\n",
    "        x_dists = x1_matches - x1[i]\n",
    "        y_dists = y1_matches - y1[i]\n",
    " \n",
    "        # computes distances of each interest point to the ground truth point\n",
    "        dists = np.sqrt(np.power(x_dists, 2.0) + np.power(y_dists, 2.0))\n",
    " \n",
    "        # get indices of points where distance is < uniqueness_dist\n",
    "        close_to_truth = dists < uniqueness_dist\n",
    " \n",
    "        # 2. get the points in image1 and their corresponding matches in image2\n",
    "        image1_x = x1_matches[close_to_truth]\n",
    "        image1_y = y1_matches[close_to_truth]\n",
    "        image2_x = x2_matches[close_to_truth]\n",
    "        image2_y = y2_matches[close_to_truth]\n",
    " \n",
    "        # 3. compute the distance of the student's image2 matches to the ground truth match\n",
    "        x_dists_2 = image2_x - x2[i]\n",
    "        y_dists_2 = image2_y - y2[i]\n",
    " \n",
    "        dists_2 = np.sqrt(np.power(x_dists_2, 2.0) + np.power(y_dists_2, 2.0))\n",
    " \n",
    "        # 4. matches within good_match_dist then count it as a correct match\n",
    "        good = dists_2 < good_match_dist\n",
    "        if np.sum(good) >= 1.0:\n",
    "            correct_matches[i] = 1\n",
    "            #good_match_counter += 1\n",
    "            if i < 100:\n",
    "                top_100_counter += 1\n",
    "        else:\n",
    "            bad_match_counter += 1\n",
    " \n",
    "    precision = (np.sum(correct_matches) / x2.shape[0]) * 100.0\n",
    "    accuracy100 = min(top_100_counter,100) # / 100) * 100# If we were testing more than the top 100, then this would be important.\n",
    " \n",
    "    print(str(np.sum(correct_matches)) + \" total good matches, \" + str(bad_match_counter) + \" total bad matches.\")\n",
    "    print(str(precision) + \"% precision\")\n",
    "    print(str(accuracy100) + \"% accuracy (top 100)\")\n",
    " \n",
    "    if vis > 0:\n",
    "        print(\"Vizualizing...\")\n",
    "        # Rescale the points to the scaled input\n",
    "        show_correspondences(img_A, img_B, \\\n",
    "                x1_est, y1_est, \\\n",
    "                x2_est, y2_est, \\\n",
    "                matches, filename)\n",
    " \n",
    "    return accuracy100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extra credit attempted\n",
    "1) Normalize the harris response R to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Interesting implementation detial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = (ix2 * iy2) - (ixy**2)\n",
    "trace = ix2 + iy2\n",
    "R = det - k*(trace**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Normalized with Min-Max method:\n",
    "\n",
    "$X=\\frac{X-X_{min}}{X_{max}-X_{min}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R= (R-np.min(R))/(np.max(R)-np.min(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Normalized with Z-score method:\n",
    "\n",
    "$X=\\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "where $\\mu$ is the mean of array and $\\sigma$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "R=stats.zscore(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Result\n",
    "1. Baseline without any normalization worked, while the accuracy is only around 30%\n",
    "![Matches image without normalization](./base-matches.jpg)\n",
    "2. Normalized with Min-Max abviously improved the result from 30% to 50%\n",
    "![Matches image with min-max normalization](./min-max-matches.jpg)\n",
    "3. Normalized with Z-score worked best, which gained 70%.\n",
    "![Matches image with z-score normalization](./matches.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
